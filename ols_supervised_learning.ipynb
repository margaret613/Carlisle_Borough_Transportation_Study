{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6873bcde",
   "metadata": {},
   "source": [
    "# Name: Margaret Nguyen\n",
    "\n",
    "# Machine Learning: Ordinary least squares (OLS)\n",
    "\n",
    "**Assignment: Create an Ordinary Least Squares model with cyclist deaths and injuries per capita as the target variable. Use per capita parameters in the model and employ data from both Massachusetts and Pennsylvania.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7158b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np # v 1.21.5\n",
    "import sklearn # v 1.0.2\n",
    "import pandas as pd # v 1.4.4\n",
    "import ydata_profiling as pp # v 3.6.6\n",
    "import statsmodels.api as sm # v 0.13.2\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Ploting libraries \n",
    "import matplotlib.pyplot as plt # v 3.5.2\n",
    "import seaborn as sns # v 0.11.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2227bb0",
   "metadata": {},
   "source": [
    "## I. df_pa_crash.csv (Pennsylvannia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca71b80c",
   "metadata": {},
   "source": [
    "### 1. Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95567659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file \n",
    "df_pa_crash = pd.read_csv('/Users/margaret06/Documents/GitHub/Carlisle_Borough_Transportation_Study/data/df_pa_crash.csv')\n",
    "\n",
    "# Clean datasets\n",
    "df_pa_crash = df_pa_crash.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "# Select columns with numeric data types (int or float) using select_dtypes\n",
    "numeric_columns = df_pa_crash.select_dtypes(include=['number'])\n",
    "\n",
    "# Create a new DataFrame with only the numeric columns\n",
    "df_pa_filtered = df_pa_crash[numeric_columns.columns]\n",
    "\n",
    "# Drop unnessary columns\n",
    "df_pa_filtered = df_pa_filtered.drop(['PENN_DOT_MUNI_ID', 'state', 'county', 'county_subdivision', 'LAND_AREA.1', 'PENN_DOT_COUNTY_NUM', 'FEDERAL_EIN_CODE', 'HOME_RULE_YEAR', 'INCORPORATION_YEAR', 'MUNICIPALITY'], axis=1)\n",
    "\n",
    "# Replace NaN values with 0 in the entire DataFrame\n",
    "df_pa_filtered = df_pa_filtered.fillna(0)\n",
    "\n",
    "# Rename BNA Score column to BNA_SCORE column\n",
    "df_pa_filtered.rename(columns={'BNA Score': 'BNA_SCORE'}, inplace=True)\n",
    "\n",
    "# Reset index\n",
    "df_pa_filtered.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e2bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for which you want to calculate per capita values\n",
    "columns_to_convert = [\n",
    "    'LAND_AREA', 'BIKE_TO_WORK_EST', 'BIKE_TO_WORK_MARG',\n",
    "    'WALK_TO_WORK_EST', 'WALK_TO_WORK_MARG', 'DRIVE_SOLO_TO_WORK_EST',\n",
    "    'DRIVE_SOLO_TO_WORK_MARG', 'CARPOOL_TO_WORK_EST',\n",
    "    'CARPOOL_TO_WORK_MARG', 'PUBTRANS_TO_WORK_EST',\n",
    "    'PUBTRANS_TO_WORK_MARG', 'EMPLOYEES_FULL_TIME',\n",
    "    'EMPLOYEES_PART_TIME', 'AUTOMOBILE_COUNT',\n",
    "    'BICYCLE_BY_AUTO_COUNT', 'BICYCLE_DEATH_BY_AUTO_COUNT',\n",
    "    'BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT', 'PED_BY_AUTO_COUNT',\n",
    "    'PED_DEATH_BY_AUTO_COUNT', 'PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT',\n",
    "    'BICYCLE_SOLO_COUNT', 'BICYCLE_DEATH_SOLO_COUNT',\n",
    "    'BICYCLE_SUSP_SERIOUS_INJ_SOLO_COUNT', 'PED_SOLO_COUNT',\n",
    "    'PED_DEATH_SOLO_COUNT', 'PED_SUSP_SERIOUS_INJ_SOLO_COUNT'\n",
    "]\n",
    "\n",
    "# Create new columns with \"_PER_CAPITA\" suffix by dividing each column by 'POPULATION'\n",
    "for column in columns_to_convert:\n",
    "    new_column_name = column + '_PER_CAPITA'\n",
    "    df_pa_filtered[new_column_name] = df_pa_filtered[column] / df_pa_filtered['POPULATION']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e356cff8",
   "metadata": {},
   "source": [
    "**Perform Exploratory Data Analysis (EDA) to check for multicollinearity among the independent variables in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9de590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pa_filtered.columns\n",
    "df_pa_filtered.shape, df_pa_filtered.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbf21b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pa_filtered.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04315a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN missing values\n",
    "df_pa_filtered.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pa_filtered.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9783fb",
   "metadata": {},
   "source": [
    "**Use Pandas Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7601c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.ProfileReport(df_pa_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42fe72c",
   "metadata": {},
   "source": [
    "**According to the Pandas Profiling report, we can see that our independent variables are highly correlated with each other. This suggests a high likelihood of multicollinearity in our linear regression model. To address this issue, I need to remove some independent variables and retain only the necessary ones as much as possible. Additionally, I have decided to remove BICYCLE_DEATH_BY_AUTO_COUNT and BICYCLE_DEATH_SOLO_COUNT because both exhibit a high level of imbalance (62.6%).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop independent variables that are highly correlated\n",
    "df_pa_filtered = df_pa_filtered.drop(['LAND_AREA', 'BIKE_TO_WORK_EST', 'BIKE_TO_WORK_MARG',\n",
    "        'WALK_TO_WORK_EST', 'WALK_TO_WORK_MARG', 'DRIVE_SOLO_TO_WORK_EST',\n",
    "        'DRIVE_SOLO_TO_WORK_MARG', 'CARPOOL_TO_WORK_EST',\n",
    "        'CARPOOL_TO_WORK_MARG', 'PUBTRANS_TO_WORK_EST', 'PUBTRANS_TO_WORK_MARG',\n",
    "        'EMPLOYEES_FULL_TIME', 'EMPLOYEES_PART_TIME', 'AUTOMOBILE_COUNT',\n",
    "        'BICYCLE_BY_AUTO_COUNT', 'BICYCLE_DEATH_BY_AUTO_COUNT',\n",
    "        'BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT', 'PED_BY_AUTO_COUNT',\n",
    "        'PED_DEATH_BY_AUTO_COUNT', 'PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT',\n",
    "        'BICYCLE_SOLO_COUNT', 'BICYCLE_DEATH_SOLO_COUNT',\n",
    "        'BICYCLE_SUSP_SERIOUS_INJ_SOLO_COUNT', 'PED_SOLO_COUNT',\n",
    "        'PED_DEATH_SOLO_COUNT', 'PED_SUSP_SERIOUS_INJ_SOLO_COUNT'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba89bee",
   "metadata": {},
   "source": [
    "### 2. Fit the linear regression using [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b91f83",
   "metadata": {},
   "source": [
    "**Separate data set in Y(independent) and X (dependent) variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f55a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_pa_filtered[\"BICYCLE_BY_AUTO_COUNT_PER_CAPITA\"] # Y = df_pa_filtered.BICYCLE_BY_AUTO_COUNT_PER_CAPITA\n",
    "X = df_pa_filtered.loc[:, df_pa_filtered.columns != \"BICYCLE_BY_AUTO_COUNT_PER_CAPITA\"] # I want all columns except the BICYCLE_BY_AUTO_COUNT_PER_CAPITA column which is the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce05ba4",
   "metadata": {},
   "source": [
    "**Use the train_test_split function to split your data into training (80%) and testing set (20%)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e91e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443547e2",
   "metadata": {},
   "source": [
    "**Fit, run or estimate the regression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a43fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression() # Create an instance of the linear regression class\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e19134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient values\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e670375",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.intercept_, model.coef_,model.score(X_test, y_test)) # Model score gives you the R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d137d925",
   "metadata": {},
   "source": [
    "**Using the independent variables in the testing set, to predict the dependent variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d57c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7947cd33",
   "metadata": {},
   "source": [
    "**Use the test set, check how well my model does in terms of error metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036bda87",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = mean_absolute_error(y_test,y_pred)\n",
    "MSE = mean_squared_error(y_test,y_pred)\n",
    "MAPE = mean_absolute_percentage_error(y_test,y_pred)\n",
    "MSE, MAE, MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e5cc45",
   "metadata": {},
   "source": [
    "### 3. Fit the linear regression using [Statsmodels](https://www.statsmodels.org/stable/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a078798",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sm.add_constant(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605053ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = sm.OLS(y_train, X_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f65e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8637cd3",
   "metadata": {},
   "source": [
    "**In-sample prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b106957",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred2 = model2.predict(X_train)\n",
    "model2.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0769af",
   "metadata": {},
   "source": [
    "**According to the results from Statsmodels:**\n",
    "- **The estimated coefficients, which include DRIVE_SOLO_TO_WORK_EST_PER_CAPITA, CARPOOL_TO_WORK_MARG_PER_CAPITA, AUTOMOBILE_COUNT_PER_CAPITA, PED_BY_AUTO_COUNT_PER_CAPITA, BICYCLE_DEATH_SOLO_COUNT_PER_CAPITA, and PED_SUSP_SERIOUS_INJ_SOLO_COUNT_PER_CAPITA, are statistically significant at the 5% level of significance in the 90% training set.**\n",
    "- **Additionally, in the 80% training set, the estimated coefficients, including DRIVE_SOLO_TO_WORK_EST_PER_CAPITA, AUTOMOBILE_COUNT_PER_CAPITA, and PED_BY_AUTO_COUNT_PER_CAPITA, are statistically significant at the 5% level of significance.**\n",
    "- **However, in the 70% training set, none of the estimated coefficients are statistically insignificant.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2ee89",
   "metadata": {},
   "source": [
    "### 4. Plot the residuals, display feature coefficients, and create a QQ plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc1ca52",
   "metadata": {},
   "source": [
    "### Credit:\n",
    "\n",
    "The following code is based on the work of my supervisor, Mitch Shiles. The original code can be found at this link: [Mitch Shiles' GitHub](https://github.com/rmshiles/Custom-Data-Tools/blob/main/testing%20for%20normality%20.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff21e3",
   "metadata": {},
   "source": [
    "**Below is a defined function designed to test residuals for a normal distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ba709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test for normality \n",
    "\n",
    "# Test for normality \n",
    "# y_test is the target variable and y_pred are the predicted variables \n",
    "def test_for_normality(y_test,y_pred):\n",
    "    from scipy.stats import boxcox\n",
    "    from scipy.stats import jarque_bera\n",
    "    from scipy.stats import normaltest\n",
    "    colo = np.random.randint(3, size=1)\n",
    "    colors=[['r','gold','c','m'],\n",
    "            ['g','orange','b','hotpink'],\n",
    "            ['skyblue','coral','lightgreen','mediumslateblue'],\n",
    "           ['g','limegreen','orange','yellow']]\n",
    "    \n",
    "    try:\n",
    "        data_series = y_pred-y_test\n",
    "    except:\n",
    "        data_series=y_test\n",
    "    # Input the mean, standard deviation and lenght of the residuals\n",
    "    normal = np.random.normal(np.mean(data_series), np.std(data_series), len(data_series))\n",
    "\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.hist(data_series,color = colors[colo[0]][1],alpha = 0.8) #bins=40,\n",
    "    plt.hist(normal,color = colors[colo[0]][2], alpha = 0.2)\n",
    "\n",
    "    # Generate a Box Plot of solar system counts\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.boxplot(data_series)\n",
    "\n",
    "    # Generate a QQ plot of the gamma distribution and the solar system counts \n",
    "    plt.subplot(2, 1, 2)\n",
    "    orderd_normal = sorted(normal)\n",
    "    ordered_data=sorted(data_series)\n",
    "    plt.scatter(ordered_data,orderd_normal, color = colors[colo[0]][3])\n",
    "    plt.plot(orderd_normal,orderd_normal,color= colors[colo[0]][0])\n",
    "    plt.title('QQPlot of residuals and normal Distribution')\n",
    "    plt.xlabel('residuals')\n",
    "    plt.ylabel('normaly distribution')\n",
    "    plt.show()\n",
    "\n",
    "    jb_stats = jarque_bera(data_series)\n",
    "    norm_stats = normaltest(data_series)\n",
    "    print('the Jarque berra stat is {}, and the pvalue is {}'.format(jb_stats[0],jb_stats[1]))\n",
    "    print(norm_stats)\n",
    "    \n",
    "    # elecResiduals = np.sort(result2_elect.resid[np.logical_not(np.isnan(result2_elect.resid))])\n",
    "    sorted_data_series = np.sort(data_series[np.logical_not(np.isnan(data_series))])\n",
    "\n",
    "    data_series_min = sorted_data_series.min()\n",
    "    data_series_max = sorted_data_series.max()\n",
    "    data_series_len = len(sorted_data_series)\n",
    "    data_series_std = np.std(sorted_data_series)\n",
    "    data_series_avg = np.mean(sorted_data_series)\n",
    "\n",
    "    print('the Minimum is {}'.format(data_series_min))\n",
    "    print('the Maximum is {}'.format(data_series_max))\n",
    "    print('the Length is {}'.format(data_series_len))\n",
    "    print('the Length is {}'.format(data_series_len))\n",
    "    print('the Standard Deviation is {}'.format(data_series_std))\n",
    "    print('the Mean is {}'.format(data_series_avg))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807baab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')  # Use an appropriate backend like 'Qt5Agg' for GUI display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b6236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the residuals, display feature coefficients, and create a QQ plot\n",
    "test_for_normality(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987837e5",
   "metadata": {},
   "source": [
    "**The warning regarding the kurtosis test indicates that it's typically valid for larger sample sizes (n >= 20). With a sample size of 9, it's recommended to use other statistical tests or methods to assess kurtosis accurately.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff6217",
   "metadata": {},
   "source": [
    "## II. df_mass_bna.csv (Massachusetts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0490b6",
   "metadata": {},
   "source": [
    "### 1. Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ca8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank dataframe\n",
    "df_mass_bna = pd.DataFrame()\n",
    "\n",
    "# Read the csv file \n",
    "df_mass_bna = pd.read_csv('/Users/margaret06/Documents/GitHub/Carlisle_Borough_Transportation_Study/data/df_mass_bna.csv')\n",
    "\n",
    "# Clean datasets\n",
    "df_mass_crash = df_mass_bna.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "# Exclude the NaN from 'VEHC_CONFIG_CL'\n",
    "df_mass_crash = df_mass_crash[df_mass_crash['VEHC_CONFIG_CL'].notna()]\n",
    "\n",
    "# List of NOT automobiles: Snowmobile, Moped, Motorcycle, Other Light Trucks (10,000 lbs., or Less), Other e.g. Farm Equipment, Unknown.\n",
    "# Exclude the non-automobiles from 'VEHC_CONFIG_CL' columns\n",
    "list_non_automobiles = ['V1:(Unknown vehicle configuration)', 'V1:(Other e.g. farm equipment)', 'V1:(Unknown vehicle configuration) / V2:(Unknown vehicle configuration)']\n",
    "df_mass_crash= df_mass_crash[~df_mass_crash['VEHC_CONFIG_CL'].isin(list_non_automobiles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e0aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fatal - injuries that resulted in death \n",
    "# Incapacitating - serious injuries require immediate medical attention\n",
    "\n",
    "## BICYCLE_DEATH_BY_AUTO_COUNT\n",
    "# Filter the DataFrame for cyclist fatalities\n",
    "cyclist_fatalities = df_mass_crash[(df_mass_crash['INJY_STAT_DESCR'] == 'Fatal injury (K)') & (df_mass_crash['NON_MTRST_TYPE_CL'] == 'Cyclist')]\n",
    "\n",
    "# Group the filtered DataFrame by 'CITY_TOWN_NAME' and calculate the count for each city\n",
    "bicycle_death_counts = cyclist_fatalities.groupby('CITY_TOWN_NAME').size().reset_index(name='BICYCLE_DEATH_BY_AUTO_COUNT')\n",
    "\n",
    "# Merge the bicycle_death_counts DataFrame into df_mass_crash on 'CITY_TOWN_NAME'\n",
    "df_mass_crash = df_mass_crash.merge(bicycle_death_counts, on='CITY_TOWN_NAME', how='left')\n",
    "\n",
    "## BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT\n",
    "cyclist_incapacitating = df_mass_crash[(df_mass_crash['INJY_STAT_DESCR'] == 'Non-fatal injury - Incapacitating') & (df_mass_crash['NON_MTRST_TYPE_CL'] == 'Cyclist')]\n",
    "bicycle_sus_serious_inj_counts = cyclist_incapacitating.groupby('CITY_TOWN_NAME').size().reset_index(name='BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT')\n",
    "\n",
    "# Merge the bicycle_sus_serious_inj_counts DataFrame into df_mass_crash on 'CITY_TOWN_NAME'\n",
    "df_mass_crash = df_mass_crash.merge(bicycle_sus_serious_inj_counts, on='CITY_TOWN_NAME', how='left')\n",
    "\n",
    "# Replace NaN values with 0 in the specified columns\n",
    "df_mass_crash['BICYCLE_DEATH_BY_AUTO_COUNT'].fillna(0, inplace=True)\n",
    "df_mass_crash['BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT'].fillna(0, inplace=True)\n",
    "\n",
    "## BICYCLE_BY_AUTO_COUNT\n",
    "df_mass_crash['BICYCLE_BY_AUTO_COUNT'] = df_mass_crash['BICYCLE_DEATH_BY_AUTO_COUNT'] + df_mass_crash['BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT']\n",
    "\n",
    "## AUTOMOBILE_COUNT\n",
    "auto_count = df_mass_crash.groupby('CITY_TOWN_NAME')['NUMB_VEHC'].sum().reset_index()\n",
    "auto_count.rename(columns={'NUMB_VEHC': 'AUTOMOBILE_COUNT'}, inplace=True)\n",
    "# Merge the auto_count into df_mass_crash on 'CITY_TOWN_NAME'\n",
    "df_mass_crash = df_mass_crash.merge(auto_count, on='CITY_TOWN_NAME', how='left')\n",
    "\n",
    "## PED_DEATH_BY_AUTO_COUNT\n",
    "# Filter the DataFrame for pedestrian fatalities\n",
    "ped_fatalities = df_mass_crash[(df_mass_crash['INJY_STAT_DESCR'] == 'Fatal injury (K)') & (df_mass_crash['NON_MTRST_TYPE_CL'] == 'Pedestrian')]\n",
    "\n",
    "# Group the filtered DataFrame by 'CITY_TOWN_NAME' and calculate the count for each city\n",
    "ped_death_counts = cyclist_fatalities.groupby('CITY_TOWN_NAME').size().reset_index(name='PED_DEATH_BY_AUTO_COUNT')\n",
    "\n",
    "# Merge the ped_death_counts DataFrame into df_mass_crash on 'CITY_TOWN_NAME'\n",
    "df_mass_crash = df_mass_crash.merge(ped_death_counts, on='CITY_TOWN_NAME', how='left')\n",
    "\n",
    "## PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT\n",
    "ped_incapacitating = df_mass_crash[(df_mass_crash['INJY_STAT_DESCR'] == 'Non-fatal injury - Incapacitating') & (df_mass_crash['NON_MTRST_TYPE_CL'] == 'Pedestrian')]\n",
    "ped_sus_serious_inj_counts = ped_incapacitating.groupby('CITY_TOWN_NAME').size().reset_index(name='PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT')\n",
    "\n",
    "# Merge the ped_sus_serious_inj_counts DataFrame into df_mass_crash on 'CITY_TOWN_NAME'\n",
    "df_mass_crash = df_mass_crash.merge(ped_sus_serious_inj_counts, on='CITY_TOWN_NAME', how='left')\n",
    "\n",
    "# Replace NaN values with 0 in the specified columns\n",
    "df_mass_crash['PED_DEATH_BY_AUTO_COUNT'].fillna(0, inplace=True)\n",
    "df_mass_crash['PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT'].fillna(0, inplace=True)\n",
    "\n",
    "##PED_BY_AUTO_COUNT\n",
    "df_mass_crash['PED_BY_AUTO_COUNT'] = df_mass_crash['PED_DEATH_BY_AUTO_COUNT'] + df_mass_crash['PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT']\n",
    "\n",
    "# Drop the duplicated rows\n",
    "df_mass_crash = df_mass_crash.drop_duplicates(subset=['CITY_TOWN_NAME', 'BNA Score', 'POPULATION', \n",
    "                                                      'BIKE_TO_WORK_EST', 'BICYCLE_BY_AUTO_COUNT', \n",
    "                                                      'BICYCLE_DEATH_BY_AUTO_COUNT', \"AUTOMOBILE_COUNT\"\n",
    "                                                      'BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT', \n",
    "                                                      'AUTOMOBILE_COUNT', 'PED_BY_AUTO_COUNT', \n",
    "                                                      'PED_DEATH_BY_AUTO_COUNT', 'PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3f87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix error\n",
    "df_mass_crash = df_mass_crash.copy()\n",
    "\n",
    "# Select columns with numeric data types (int or float) using select_dtypes\n",
    "numeric_columns = df_mass_crash.select_dtypes(include=['number'])\n",
    "\n",
    "# Create a new DataFrame with only the numeric columns\n",
    "df_mass_crash = df_mass_crash[numeric_columns.columns]\n",
    "\n",
    "# Rename BNA Score column to BNA_SCORE column\n",
    "df_mass_crash.rename(columns={'BNA Score': 'BNA_SCORE'}, inplace=True)\n",
    "\n",
    "# Reset index\n",
    "df_mass_crash.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6cf23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for which you want to calculate per capita values\n",
    "columns_to_convert = [\n",
    "    'BIKE_TO_WORK_EST', 'BIKE_TO_WORK_MARG',\n",
    "    'WALK_TO_WORK_EST', 'WALK_TO_WORK_MARG', 'DRIVE_SOLO_TO_WORK_EST',\n",
    "    'DRIVE_SOLO_TO_WORK_MARG', 'CARPOOL_TO_WORK_EST',\n",
    "    'CARPOOL_TO_WORK_MARG', 'PUBTRANS_TO_WORK_EST',\n",
    "    'PUBTRANS_TO_WORK_MARG', 'AUTOMOBILE_COUNT',\n",
    "    'BICYCLE_BY_AUTO_COUNT', 'BICYCLE_DEATH_BY_AUTO_COUNT',\n",
    "    'BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT', 'PED_BY_AUTO_COUNT',\n",
    "    'PED_DEATH_BY_AUTO_COUNT', 'PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT']\n",
    "\n",
    "# Create new columns with \"_PER_CAPITA\" suffix by dividing each column by 'POPULATION'\n",
    "for column in columns_to_convert:\n",
    "    new_column_name = column + '_PER_CAPITA'\n",
    "    df_mass_crash[new_column_name] = df_mass_crash[column] / df_mass_crash['POPULATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed343d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnessary columns\n",
    "df_mass_crash = df_mass_crash.drop(['OBJECTID', 'CRASH_NUMB', 'NUMB_VEHC', 'NUMB_NONFATAL_INJR',\n",
    "       'NUMB_FATAL_INJR', 'MILEMARKER', 'X', 'Y', 'LAT', 'LON', 'YEAR',\n",
    "       'DISTRICT_NUM', 'SPEED_LIMIT', 'AADT', 'AADT_YEAR', 'PK_PCT_SUT',\n",
    "       'AV_PCT_SUT', 'PK_PCT_CT', 'AV_PCT_CT', 'LT_SIDEWLK', 'RT_SIDEWLK',\n",
    "       'SHLDR_LT_W', 'SURFACE_WD', 'SHLDR_RT_W', 'NUM_LANES', 'OPP_LANES',\n",
    "       'MED_WIDTH', 'PEAK_LANE', 'SPEED_LIM', 'STATN_NUM', 'OP_DIR_SL',\n",
    "       'SHLDR_UL_W', 'VEHC_UNIT_NUMB', 'DRIVER_AGE', 'TOTAL_OCCPT_IN_VEHC',\n",
    "       'PERS_NUMB', 'AGE', 'state', 'county', 'county_subdivision'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa446bb2",
   "metadata": {},
   "source": [
    "**Perform Exploratory Data Analysis (EDA) to check for multicollinearity among the independent variables in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee14bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN missing values\n",
    "df_mass_crash.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9044ea8a",
   "metadata": {},
   "source": [
    "**Use Pandas Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f41854",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.ProfileReport(df_mass_crash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1107f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop independent variables that are highly correlated\n",
    "df_mass_filtered = df_mass_crash.drop(['BIKE_TO_WORK_EST', 'BIKE_TO_WORK_MARG',\n",
    "       'WALK_TO_WORK_EST', 'WALK_TO_WORK_MARG', 'DRIVE_SOLO_TO_WORK_EST',\n",
    "       'DRIVE_SOLO_TO_WORK_MARG', 'CARPOOL_TO_WORK_EST',\n",
    "       'CARPOOL_TO_WORK_MARG', 'PUBTRANS_TO_WORK_EST', 'PUBTRANS_TO_WORK_MARG',\n",
    "       'BNA_SCORE', 'BICYCLE_DEATH_BY_AUTO_COUNT',\n",
    "       'BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT', 'BICYCLE_BY_AUTO_COUNT',\n",
    "       'AUTOMOBILE_COUNT', 'PED_DEATH_BY_AUTO_COUNT',\n",
    "       'PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT', 'PED_BY_AUTO_COUNT'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca740c",
   "metadata": {},
   "source": [
    "### 2. Fit the linear regression using [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fe94e8",
   "metadata": {},
   "source": [
    "**Separate data set in Y(independent) and X (dependent) variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa34de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = df_mass_filtered[\"BICYCLE_BY_AUTO_COUNT_PER_CAPITA\"] # Y = df_mass_filtered.BICYCLE_BY_AUTO_COUNT_PER_CAPITA\n",
    "X2 = df_mass_filtered.loc[:, df_mass_filtered.columns != \"BICYCLE_BY_AUTO_COUNT_PER_CAPITA\"] # I want all columns except the BICYCLE_BY_AUTO_COUNT_PER_CAPITA column which is the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d599d2",
   "metadata": {},
   "source": [
    "**Use the train_test_split function to split your data into training (80%) and testing set (20%)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebdcb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X2, y2, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92030b",
   "metadata": {},
   "source": [
    "**Fit, run or estimate the regression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e5b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression() # Create an instance of the linear regression class\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cf92ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db716f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.intercept_, model.coef_,model.score(X_test, y_test)) # Model score gives you the R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e31872d",
   "metadata": {},
   "source": [
    "**Use the independent variables in the testing set, to predict the dependent variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6481814",
   "metadata": {},
   "source": [
    "**Use the test set, check how well the model does in terms of error metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6546cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = mean_absolute_error(y_test,y_pred)\n",
    "MSE = mean_squared_error(y_test,y_pred)\n",
    "MAPE =  mean_absolute_percentage_error(y_test,y_pred)\n",
    "MSE, MAE, MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba50abe9",
   "metadata": {},
   "source": [
    "### 3. Fit the linear regression using [Statsmodels](https://www.statsmodels.org/stable/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76325e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sm.add_constant(X_train)\n",
    "model2 = sm.OLS(y_train, X_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271f5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54b09dd",
   "metadata": {},
   "source": [
    "**According to the results from Statsmodels, in the 80% training set, the estimated coefficients, including BICYCLE_DEATH_BY_AUTO_COUNT_PER_CAPITA, BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT_PER_CAPITA, PED_BY_AUTO_COUNT_PER_CAPITA, PED_DEATH_BY_AUTO_COUNT_PER_CAPITA, PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT_PER_CAPITA,  are statistically significant at the 5% level of significance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25650c66",
   "metadata": {},
   "source": [
    "**In-sample prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0878c1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred2 = model2.predict(X_train)\n",
    "model2.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dd9c0c",
   "metadata": {},
   "source": [
    "### 4. Plot the residuals, display feature coefficients, and create a QQ plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c30c0",
   "metadata": {},
   "source": [
    "### Credit:\n",
    "\n",
    "The following code is based on the work of my supervisor, Mitch Shiles. The original code can be found at this link: [Mitch Shiles' GitHub](https://github.com/rmshiles/Custom-Data-Tools/blob/main/testing%20for%20normality%20.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the residuals, display feature coefficients, and create a QQ plot\n",
    "test_for_normality(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dff279a",
   "metadata": {},
   "source": [
    "**The warning regarding the kurtosis test indicates that it's typically valid for larger sample sizes (n >= 20). With a sample size of 9, it's recommended to use other statistical tests or methods to assess kurtosis accurately.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
