{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6873bcde",
   "metadata": {},
   "source": [
    "## Name: Margaret Nguyen\n",
    "\n",
    "# Exploring the Impact of Infrastructure on Incident Rates for Vulnerable Road Users: A Machine Learning Analysis\n",
    "\n",
    "**Assignment: Use the 'POPULATION' variable to conduct stepwise regression. This coding file is intended for the dataset without BNA score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bd7158b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np # v 1.21.5\n",
    "import sklearn # v 1.0.2\n",
    "import pandas as pd # v 1.4.4\n",
    "import ydata_profiling as pp # v 3.6.6\n",
    "import statsmodels.api as sm # v 0.13.2\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector # v 0.23.0\n",
    "\n",
    "# Regression\n",
    "from sklearn.linear_model import LinearRegression # v 1.0.2\n",
    "from sklearn.model_selection import train_test_split # v 1.0.2\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA # v 1.0.2\n",
    "from sklearn.preprocessing import StandardScaler # v 1.0.2\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error # v 1.0.2\n",
    "from sklearn.metrics import mean_squared_error # v 1.0.2\n",
    "from sklearn.metrics import mean_absolute_percentage_error # v 1.0.2\n",
    "\n",
    "# Ploting libraries \n",
    "import matplotlib\n",
    "#matplotlib.use('Qt5Agg')  # Use an appropriate backend like 'Qt5Agg' for GUI display\n",
    "import matplotlib.pyplot as plt # v 3.5.2\n",
    "import seaborn as sns # v 0.11.2\n",
    "# Display any generated plots or visualizations directly in the notebook interface\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca71b80c",
   "metadata": {},
   "source": [
    "# I. Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "05cd8088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file \n",
    "df_pa_acs = pd.read_csv('/Users/margaret06/Documents/GitHub/Carlisle_Borough_Transportation_Study/data/2017_TO_2021_MUNI_CRASH_DATA.csv')\n",
    "\n",
    "df_mass_acs = pd.read_csv('/Users/margaret06/Documents/GitHub/Carlisle_Borough_Transportation_Study/data/df_mass_acs.csv', low_memory=False)\n",
    "\n",
    "df_col_acs = pd.read_csv('/Users/margaret06/Documents/GitHub/Carlisle_Borough_Transportation_Study/data/df_col_acs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5bd0c4",
   "metadata": {},
   "source": [
    "## A. Clean the Pennsylvania Crash Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "95567659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean datasets\n",
    "df_pa_acs.drop(columns = ['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Select columns with numeric data types (int or float) using select_dtypes\n",
    "numeric_columns = df_pa_acs.select_dtypes(include=['number'])\n",
    "\n",
    "# Create a new DataFrame with only the numeric columns\n",
    "df_pa_crash = df_pa_acs[numeric_columns.columns]\n",
    "\n",
    "# Drop unnessary columns\n",
    "df_pa_crash = df_pa_crash.drop(['PENN_DOT_MUNI_ID', 'state', 'county', 'county_subdivision', 'LAND_AREA.1', 'LAND_AREA', 'PENN_DOT_COUNTY_NUM', 'FEDERAL_EIN_CODE', 'HOME_RULE_YEAR', 'INCORPORATION_YEAR', 'MUNICIPALITY'], axis=1)\n",
    "\n",
    "# Replace NaN values with 0 throughout the DataFrame to address missing data, following the experience of cleaning the original dataset.\n",
    "df_pa_crash = df_pa_crash.fillna(0)\n",
    "\n",
    "# Drop population == 0 (small municipalities)\n",
    "df_pa_crash = df_pa_crash[df_pa_crash['POPULATION'] != 0]\n",
    "\n",
    "# Reset index\n",
    "df_pa_crash.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "131e2bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for which you want to calculate per capita values\n",
    "columns_to_convert = [\n",
    "    'BIKE_TO_WORK_EST', 'BIKE_TO_WORK_MARG',\n",
    "    'WALK_TO_WORK_EST', 'WALK_TO_WORK_MARG', 'DRIVE_SOLO_TO_WORK_EST',\n",
    "    'DRIVE_SOLO_TO_WORK_MARG', 'CARPOOL_TO_WORK_EST',\n",
    "    'CARPOOL_TO_WORK_MARG', 'PUBTRANS_TO_WORK_EST',\n",
    "    'PUBTRANS_TO_WORK_MARG', 'EMPLOYEES_FULL_TIME',\n",
    "    'EMPLOYEES_PART_TIME', 'AUTOMOBILE_COUNT',\n",
    "    'BICYCLE_BY_AUTO_COUNT', 'BICYCLE_DEATH_BY_AUTO_COUNT',\n",
    "    'BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT', 'PED_BY_AUTO_COUNT',\n",
    "    'PED_DEATH_BY_AUTO_COUNT', 'PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT',\n",
    "    'BICYCLE_SOLO_COUNT', 'BICYCLE_DEATH_SOLO_COUNT',\n",
    "    'BICYCLE_SUSP_SERIOUS_INJ_SOLO_COUNT', 'PED_SOLO_COUNT',\n",
    "    'PED_DEATH_SOLO_COUNT', 'PED_SUSP_SERIOUS_INJ_SOLO_COUNT',\n",
    "]\n",
    "\n",
    "# Create new columns with \"_PER_CAPITA\" suffix by dividing each column by 'POPULATION'\n",
    "for column in columns_to_convert:\n",
    "    new_column_name = column + '_PER_CAPITA'\n",
    "    df_pa_crash[new_column_name] = df_pa_crash[column] / df_pa_crash['POPULATION']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ec14e0",
   "metadata": {},
   "source": [
    "## B. Clean the Massachusett Crash Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a88071b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean datasets\n",
    "df_mass_acs.drop(columns = ['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Exclude the NaN from 'VEHC_CONFIG_CL'\n",
    "df_mass_crash = df_mass_acs[df_mass_acs['VEHC_CONFIG_CL'].notna()]\n",
    "\n",
    "# List of NOT automobiles: Snowmobile, Moped, Motorcycle, Other Light Trucks (10,000 lbs., or Less), Other e.g. Farm Equipment, Unknown.\n",
    "# Exclude the non-automobiles from 'VEHC_CONFIG_CL' columns\n",
    "list_non_automobiles = ['V1:(Unknown vehicle configuration)', 'V1:(Other e.g. farm equipment)', 'V1:(Unknown vehicle configuration) / V2:(Unknown vehicle configuration)']\n",
    "df_mass_crash= df_mass_crash[~df_mass_crash['VEHC_CONFIG_CL'].isin(list_non_automobiles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7055f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fatal - injuries that resulted in death \n",
    "# Incapacitating - serious injuries require immediate medical attention\n",
    "\n",
    "## BICYCLE_DEATH_BY_AUTO_COUNT\n",
    "# Filter the DataFrame for cyclist fatalities\n",
    "cyclist_fatalities = df_mass_crash[(df_mass_crash['INJY_STAT_DESCR'] == 'Fatal injury (K)') & (df_mass_crash['NON_MTRST_TYPE_CL'] == 'Cyclist')]\n",
    "\n",
    "# Group the filtered DataFrame by 'CITY_TOWN_NAME' and calculate the count for each city\n",
    "bicycle_death_counts = cyclist_fatalities.groupby('CITY_TOWN_NAME').size().reset_index(name='BICYCLE_DEATH_BY_AUTO_COUNT')\n",
    "\n",
    "# Merge the bicycle_death_counts DataFrame into df_mass_crash on 'CITY_TOWN_NAME'\n",
    "df_mass_crash = df_mass_crash.merge(bicycle_death_counts, on='CITY_TOWN_NAME', how='left')\n",
    "\n",
    "## BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT\n",
    "cyclist_incapacitating = df_mass_crash[(df_mass_crash['INJY_STAT_DESCR'] == 'Non-fatal injury - Incapacitating') & (df_mass_crash['NON_MTRST_TYPE_CL'] == 'Cyclist')]\n",
    "bicycle_sus_serious_inj_counts = cyclist_incapacitating.groupby('CITY_TOWN_NAME').size().reset_index(name='BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT')\n",
    "\n",
    "# Merge the bicycle_sus_serious_inj_counts DataFrame into df_mass_crash on 'CITY_TOWN_NAME'\n",
    "df_mass_crash = df_mass_crash.merge(bicycle_sus_serious_inj_counts, on='CITY_TOWN_NAME', how='left')\n",
    "\n",
    "# Replace NaN values with 0 in the specified columns\n",
    "df_mass_crash['BICYCLE_DEATH_BY_AUTO_COUNT'].fillna(0, inplace=True)\n",
    "df_mass_crash['BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT'].fillna(0, inplace=True)\n",
    "\n",
    "## BICYCLE_BY_AUTO_COUNT\n",
    "df_mass_crash['BICYCLE_BY_AUTO_COUNT'] = df_mass_crash['BICYCLE_DEATH_BY_AUTO_COUNT'] + df_mass_crash['BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT']\n",
    "\n",
    "## AUTOMOBILE_COUNT\n",
    "auto_count = df_mass_crash.groupby('CITY_TOWN_NAME')['NUMB_VEHC'].sum().reset_index()\n",
    "auto_count.rename(columns={'NUMB_VEHC': 'AUTOMOBILE_COUNT'}, inplace=True)\n",
    "# Merge the auto_count into df_mass_crash on 'CITY_TOWN_NAME'\n",
    "df_mass_crash = df_mass_crash.merge(auto_count, on='CITY_TOWN_NAME', how='left')\n",
    "\n",
    "## PED_DEATH_BY_AUTO_COUNT\n",
    "# Filter the DataFrame for pedestrian fatalities\n",
    "ped_fatalities = df_mass_crash[(df_mass_crash['INJY_STAT_DESCR'] == 'Fatal injury (K)') & (df_mass_crash['NON_MTRST_TYPE_CL'] == 'Pedestrian')]\n",
    "\n",
    "# Group the filtered DataFrame by 'CITY_TOWN_NAME' and calculate the count for each city\n",
    "ped_death_counts = cyclist_fatalities.groupby('CITY_TOWN_NAME').size().reset_index(name='PED_DEATH_BY_AUTO_COUNT')\n",
    "\n",
    "# Merge the ped_death_counts DataFrame into df_mass_crash on 'CITY_TOWN_NAME'\n",
    "df_mass_crash = df_mass_crash.merge(ped_death_counts, on='CITY_TOWN_NAME', how='left')\n",
    "\n",
    "## PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT\n",
    "ped_incapacitating = df_mass_crash[(df_mass_crash['INJY_STAT_DESCR'] == 'Non-fatal injury - Incapacitating') & (df_mass_crash['NON_MTRST_TYPE_CL'] == 'Pedestrian')]\n",
    "ped_sus_serious_inj_counts = ped_incapacitating.groupby('CITY_TOWN_NAME').size().reset_index(name='PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT')\n",
    "\n",
    "# Merge the ped_sus_serious_inj_counts DataFrame into df_mass_crash on 'CITY_TOWN_NAME'\n",
    "df_mass_crash = df_mass_crash.merge(ped_sus_serious_inj_counts, on='CITY_TOWN_NAME', how='left')\n",
    "\n",
    "# Replace NaN values with 0 in the specified columns\n",
    "df_mass_crash['PED_DEATH_BY_AUTO_COUNT'].fillna(0, inplace=True)\n",
    "df_mass_crash['PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT'].fillna(0, inplace=True)\n",
    "\n",
    "##PED_BY_AUTO_COUNT\n",
    "df_mass_crash['PED_BY_AUTO_COUNT'] = df_mass_crash['PED_DEATH_BY_AUTO_COUNT'] + df_mass_crash['PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT']\n",
    "\n",
    "# Drop the duplicated rows\n",
    "df_mass_crash = df_mass_crash.drop_duplicates(subset=['CITY_TOWN_NAME', 'POPULATION', \n",
    "                                                      'BIKE_TO_WORK_EST', 'BICYCLE_BY_AUTO_COUNT', \n",
    "                                                      'BICYCLE_DEATH_BY_AUTO_COUNT', \n",
    "                                                      'BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT', \n",
    "                                                      'AUTOMOBILE_COUNT', 'PED_BY_AUTO_COUNT', \n",
    "                                                      'PED_DEATH_BY_AUTO_COUNT', 'PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70db9a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns with numeric data types (int or float) using select_dtypes\n",
    "numeric_columns = df_mass_crash.select_dtypes(include=['number'])\n",
    "\n",
    "# Create a new DataFrame with only the numeric columns\n",
    "df_mass_crash = df_mass_crash[numeric_columns.columns]\n",
    "\n",
    "# Reset index\n",
    "df_mass_crash.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ea8b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for which you want to calculate per capita values\n",
    "columns_to_convert = [\n",
    "    'BIKE_TO_WORK_EST', 'BIKE_TO_WORK_MARG',\n",
    "    'WALK_TO_WORK_EST', 'WALK_TO_WORK_MARG', 'DRIVE_SOLO_TO_WORK_EST',\n",
    "    'DRIVE_SOLO_TO_WORK_MARG', 'CARPOOL_TO_WORK_EST',\n",
    "    'CARPOOL_TO_WORK_MARG', 'PUBTRANS_TO_WORK_EST',\n",
    "    'PUBTRANS_TO_WORK_MARG', 'AUTOMOBILE_COUNT',\n",
    "    'BICYCLE_BY_AUTO_COUNT', 'BICYCLE_DEATH_BY_AUTO_COUNT',\n",
    "    'BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT', 'PED_BY_AUTO_COUNT',\n",
    "    'PED_DEATH_BY_AUTO_COUNT', 'PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT']\n",
    "\n",
    "# Create new columns with \"_PER_CAPITA\" suffix by dividing each column by 'POPULATION'\n",
    "for column in columns_to_convert:\n",
    "    new_column_name = column + '_PER_CAPITA'\n",
    "    df_mass_crash[new_column_name] = df_mass_crash[column] / df_mass_crash['POPULATION']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d8607",
   "metadata": {},
   "source": [
    "## C. Clean the Colorado Crash Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c417a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean datasets\n",
    "df_col_acs.drop(columns = ['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f361e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nake a copy of df_col_acs\n",
    "df_col_crash = df_col_acs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90759d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for which you want to calculate per capita values\n",
    "columns_to_convert = [\n",
    "    'BIKE_TO_WORK_EST', 'BIKE_TO_WORK_MARG',\n",
    "    'WALK_TO_WORK_EST', 'WALK_TO_WORK_MARG', 'DRIVE_SOLO_TO_WORK_EST',\n",
    "    'DRIVE_SOLO_TO_WORK_MARG', 'CARPOOL_TO_WORK_EST',\n",
    "    'CARPOOL_TO_WORK_MARG', 'PUBTRANS_TO_WORK_EST',\n",
    "    'PUBTRANS_TO_WORK_MARG', \n",
    "    'BICYCLE_BY_AUTO_COUNT', 'BICYCLE_DEATH_BY_AUTO_COUNT',\n",
    "    'BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT', 'PED_BY_AUTO_COUNT',\n",
    "    'PED_DEATH_BY_AUTO_COUNT', 'PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT']\n",
    "\n",
    "# Create new columns with \"_PER_CAPITA\" suffix by dividing each column by 'POPULATION'\n",
    "for column in columns_to_convert:\n",
    "    new_column_name = column + '_PER_CAPITA'\n",
    "    df_col_crash[new_column_name] = df_col_crash[column] / df_col_crash['POPULATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ea2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the STATE coloumn\n",
    "df_pa_crash['STATE'] = 'PA'\n",
    "df_mass_crash['STATE'] = 'MA'\n",
    "df_col_crash['STATE'] = 'CO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b058e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the common columns\n",
    "common_columns = list(set(df_pa_crash.columns) & set(df_col_crash.columns) & set(df_mass_crash.columns))\n",
    "\n",
    "# Keep to have the same columns\n",
    "df_mass_crash = df_mass_crash[common_columns]\n",
    "df_pa_crash = df_pa_crash[common_columns]\n",
    "df_col_crash = df_col_crash[common_columns]\n",
    "\n",
    "# Reorder the columns of the df_mass_crash and df_col_crash to match df_pa_crash\n",
    "df_mass_crash = df_mass_crash[df_pa_crash.columns]\n",
    "df_col_crash = df_col_crash[df_pa_crash.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006aab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of each datadrame\n",
    "df_mass_crash.shape, df_pa_crash.shape, df_col_crash.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2f721",
   "metadata": {},
   "source": [
    "## C. Merge df_pa_crash and df_mass_crash Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef18be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes\n",
    "crash_acs = pd.concat([df_pa_crash, df_mass_crash], axis=0)\n",
    "df_crash_acs = pd.concat([crash_acs, df_col_crash], axis=0)\n",
    "\n",
    "# Reset index\n",
    "df_crash_acs.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Check for the shape of the datafrane\n",
    "print(df_crash_acs.shape)\n",
    "\n",
    "# Show merge dataframe\n",
    "df_crash_acs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b21ccf",
   "metadata": {},
   "source": [
    "**Create dumnmy variable for each dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6271bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crash_acs = pd.get_dummies(df_crash_acs, columns=['STATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04315a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN missing values\n",
    "df_crash_acs.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9783fb",
   "metadata": {},
   "source": [
    "**Use Pandas Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7601c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp.ProfileReport(df_crash_acs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3333733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_crash_acs is your DataFrame\n",
    "selected_columns = [\n",
    "    'CARPOOL_TO_WORK_MARG_PER_CAPITA', \n",
    "    'PED_BY_AUTO_COUNT_PER_CAPITA', \n",
    "    'PED_DEATH_BY_AUTO_COUNT_PER_CAPITA', \n",
    "    'DRIVE_SOLO_TO_WORK_MARG_PER_CAPITA', \n",
    "    'BICYCLE_DEATH_BY_AUTO_COUNT_PER_CAPITA', \n",
    "    'WALK_TO_WORK_MARG_PER_CAPITA', \n",
    "    'WALK_TO_WORK_EST_PER_CAPITA', \n",
    "    'CARPOOL_TO_WORK_EST_PER_CAPITA', \n",
    "    'DRIVE_SOLO_TO_WORK_EST_PER_CAPITA', \n",
    "    'BICYCLE_BY_AUTO_COUNT_PER_CAPITA', \n",
    "    'PUBTRANS_TO_WORK_EST_PER_CAPITA', \n",
    "    'BICYCLE_SUSP_SERIOUS_INJ_BY_AUTO_COUNT_PER_CAPITA', \n",
    "    'BIKE_TO_WORK_EST_PER_CAPITA', \n",
    "    'PUBTRANS_TO_WORK_MARG_PER_CAPITA', \n",
    "    'BIKE_TO_WORK_MARG_PER_CAPITA', \n",
    "    'PED_SUSP_SERIOUS_INJ_BY_AUTO_COUNT_PER_CAPITA', \n",
    "    'STATE_CO', \n",
    "    'STATE_MA',\n",
    "    'STATE_PA'\n",
    "]\n",
    "\n",
    "# Select the specified columns from the DataFrame\n",
    "df_crash_acs = df_crash_acs[selected_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563466f4",
   "metadata": {},
   "source": [
    "# II. Stepwise Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b91f83",
   "metadata": {},
   "source": [
    "## A. Separate Dataset in Y (independent) and X (dependent) Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64709a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_crash_acs[\"BICYCLE_BY_AUTO_COUNT_PER_CAPITA\"] # Y = df_crash.BICYCLE_BY_AUTO_COUNT_PER_CAPITA\n",
    "X = df_crash_acs.drop(columns=\"BICYCLE_BY_AUTO_COUNT_PER_CAPITA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553674cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = df_crash_acs[\"PED_BY_AUTO_COUNT_PER_CAPITA\"] # Y = df_crash.PED_BY_AUTO_COUNT_PER_CAPITA\n",
    "X2 = df_crash_acs.drop(columns=\"PED_BY_AUTO_COUNT_PER_CAPITA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce05ba4",
   "metadata": {},
   "source": [
    "## B. Use the train_test_split Function to Split Data into Training (80%) and Testing Set (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecacf086",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc4ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443547e2",
   "metadata": {},
   "source": [
    "## C. Fit, Run or Estimate the Regression Model\n",
    "\n",
    "### 1. BICYCLE_BY_AUTO_COUNT_PER_CAPITA as the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eec382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear regression model\n",
    "OLS = LinearRegression()\n",
    "\n",
    "# Forward feature selection using SequentialFeatureSelector\n",
    "sfs = SequentialFeatureSelector(OLS, k_features=\"best\", forward=True, scoring='neg_mean_squared_error', cv=10)\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "# Selected features\n",
    "selected_features = list(X_train.columns[list(sfs.k_feature_idx_)])\n",
    "print(\"Selected Features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e01b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the selected features\n",
    "selected_features = list(X_train.columns[list(sfs.k_feature_idx_)])\n",
    "\n",
    "# Build a linear regression model using statsmodels with the selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_train_selected = sm.add_constant(X_train_selected)  # Add a constant for the intercept\n",
    "model = sm.OLS(y_train, X_train_selected).fit()\n",
    "\n",
    "# Display the summary of the statsmodels linear regression model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bef7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "X_test_selected = X_test[selected_features]\n",
    "X_test_selected = sm.add_constant(X_test_selected) # Add a constant term to the test data\n",
    "y_pred = model.predict(X_test_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa3216",
   "metadata": {},
   "source": [
    "### 2. PED_BY_AUTO_COUNT_PER_CAPITA as the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50403433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear regression model\n",
    "OLS = LinearRegression()\n",
    "\n",
    "# Forward feature selection using SequentialFeatureSelector\n",
    "sfs = SequentialFeatureSelector(OLS, k_features=\"best\", forward=True, scoring='neg_mean_squared_error', cv=10)\n",
    "sfs.fit(X_train2, y_train2)\n",
    "\n",
    "# Selected features\n",
    "selected_features2 = list(X_train2.columns[list(sfs.k_feature_idx_)])\n",
    "print(\"Selected Features:\", selected_features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e69bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the selected features\n",
    "selected_features2 = list(X_train2.columns[list(sfs.k_feature_idx_)])\n",
    "\n",
    "# Build a linear regression model using statsmodels with the selected features\n",
    "X_train_selected2 = X_train2[selected_features2]\n",
    "X_train_selected2 = sm.add_constant(X_train_selected2)  # Add a constant for the intercept\n",
    "model2 = sm.OLS(y_train2, X_train_selected2).fit()\n",
    "\n",
    "# Display the summary of the statsmodels linear regression model\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45282ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "X_test_selected2 = X_test2[selected_features2]\n",
    "X_test_selected2 = sm.add_constant(X_test_selected2) # Add a constant term to the test data\n",
    "y_pred2 = model2.predict(X_test_selected2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c56d395",
   "metadata": {},
   "source": [
    "## D. Plot the Residuals\n",
    "\n",
    "### Graph 1: The residuals for the model with 'BICYCLE_BY_AUTO_COUNT_PER_CAPITA' as the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368c9cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = model.resid\n",
    "\n",
    "plt.scatter(model.fittedvalues, residuals)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"OLS Residual Plot for Cyclists\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add a horizontal line at y=0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a50ffc",
   "metadata": {},
   "source": [
    "### Graph 2: The residuals for the model with 'PED_BY_AUTO_COUNT_PER_CAPITA' as the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals2 = model2.resid\n",
    "\n",
    "plt.scatter(model2.fittedvalues, residuals2)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"OLS Residual Plot for Pedestrains\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # Add a horizontal line at y=0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a675f1e3",
   "metadata": {},
   "source": [
    "## E. Plot the Actual vs. Predicted Values\n",
    "\n",
    "### Graph 3: The Actual vs. Predicted Values for the model with 'BICYCLE_BY_AUTO_COUNT_PER_CAPITA' as the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe7c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual vs. predicted values\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values for Cyclists\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef06a1b0",
   "metadata": {},
   "source": [
    "### Graph 4: The Actual vs. Predicted Values for the model with 'PED_BY_AUTO_COUNT_PER_CAPITA' as the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f12863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual vs. predicted values\n",
    "plt.scatter(y_test2, y_pred2)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values for Pedestrians\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
